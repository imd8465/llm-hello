{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJXW_DgiSebM"
   },
   "source": [
    "# Agentic RAG Powered by LangChain\n",
    "\n",
    "In the following notebook, we'll be taking a look at how to build Agentic RAG Applications with LangChain.\n",
    "\n",
    "We'll be relying on a few great tools to help us do this:\n",
    "\n",
    "1. LangChain - more specifically LCEL\n",
    "2. LangGraph\n",
    "3. LangSmith\n",
    "\n",
    "Let's get started with a quick overview of LCEL and build a simple RAG chain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fLDElOVoop"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "We'll first install all our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaVwN269EttM",
    "outputId": "2d76b2bf-d145-4f80-d842-23e3bc996622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBvTZ-JshKuo",
    "outputId": "e221e0bd-7d84-46ab-8a47-ad84c7579e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU faiss-cpu pymupdf langchain-community wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wujPjGJuoPwg"
   },
   "source": [
    "## Environment Variables\n",
    "\n",
    "We'll want to set both our OpenAI API key and our LangSmith environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdh8CoVWHRvs",
    "outputId": "1332fa2d-9c2e-4845-d6be-a9f838977172"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv0glIDyHmRt",
    "outputId": "abc68d0e-d2f3-42bb-a2d0-65a5bba3d45d"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"imd8465 - LangGraph - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-CufnKBfFMq"
   },
   "source": [
    "## Initialize a Simple Chain using LCEL\n",
    "\n",
    "The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnuLQnqzg0DZ"
   },
   "source": [
    "### Retrieval\n",
    "\n",
    "First, we'll set up a simple local retriever system that looks at Arxiv papers on the topic of Function Calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g8rnnJbJg8sB"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "docs = ArxivLoader(query=\"Function Calling\", load_max_docs=5).load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    ")\n",
    "\n",
    "retriever = faiss_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-rSWYKjinCA"
   },
   "source": [
    "### Augmented\n",
    "\n",
    "Now that we have our retrieval system ready to rock, we can create our RAG prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gZudebB0it20"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPVEVKmPfoYs"
   },
   "source": [
    "### Generation\n",
    "\n",
    "Let's start by initializing our model. In this case, we'll be using OpenAI's `gpt-3.5-turbo` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iAU6ilyPfufs"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhxJkDIujhC-"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "Now that we have our R, A, and G components - let's build our simple RAG chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z4itzKufj-OI"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_generation_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | openai_chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4vr1n-7kN-y"
   },
   "source": [
    "And let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocoqybsjkPXz",
    "outputId": "227ab541-9f47-4941-a2df-b84f6b7a0c26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': AIMessage(content='Function Calling in the context of AI refers to the process where large language models (LLMs) generate and execute calls to interface with external tools and data sources. This interaction is typically synchronous, meaning each call blocks LLM inference until completion, limiting operational efficiency and concurrent function execution. To address this limitation, the concept of asynchronous LLM function calling has been introduced, allowing LLMs to generate and execute function calls concurrently. AsyncLM, a system for asynchronous LLM function calling, incorporates interrupt mechanisms to notify the LLM when function calls return, enabling more efficient task completion.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 2660, 'total_tokens': 2777, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9a78949c-92e4-4b80-bfa2-e73ce014103e-0', usage_metadata={'input_tokens': 2660, 'output_tokens': 117, 'total_tokens': 2777, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'context': [Document(id='5da48e04-1d8c-4912-9a0a-9562151a5777', metadata={'Published': '2024-12-09', 'Title': 'Asynchronous LLM Function Calling', 'Authors': 'In Gim, Seung-seob Lee, Lin Zhong', 'Summary': \"Large language models (LLMs) use function calls to interface with external\\ntools and data source. However, the current approach to LLM function calling is\\ninherently synchronous, where each call blocks LLM inference, limiting LLM\\noperation and concurrent function execution. In this work, we propose AsyncLM,\\na system for asynchronous LLM function calling. AsyncLM improves LLM's\\noperational efficiency by enabling LLMs to generate and execute function calls\\nconcurrently. Instead of waiting for each call's completion, AsyncLM introduces\\nan interrupt mechanism to asynchronously notify the LLM in-flight when function\\ncalls return. We design an in-context protocol for function calls and\\ninterrupts, provide fine-tuning strategy to adapt LLMs to the interrupt\\nsemantics, and implement these mechanisms efficiently on LLM inference process.\\nWe demonstrate that AsyncLM can reduce end-to-end task completion latency from\\n1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks\\nin the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss\\nhow interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM\\ninteractions.\"}, page_content='ability is refined either through fine-tuning on diverse task\\ndatasets (Patil et al., 2024) or using in-context instruc-\\ntions (Liang et al., 2024). Function descriptions, includ-\\ning arguments, are provided in a structured prompt format,\\noften in JSON. Our approach builds on this paradigm by\\nenabling asynchronous function calling, requiring LLMs to\\n(1) consider execution time in call generation and (2) use\\ninterrupt semantics to decide subsequent calls.\\nEfficient LLM function calling. A major challenge in func-\\ntion calling is optimizing efficiency to enhance resource uti-\\nlization and reduce latency. Various studies have explored\\ndifferent optimization strategies to improve the function\\ncalling process. For instance, parallel function calling ap-\\nproaches (Kim et al., 2023; OpenAI, 2023) instruct the\\nLLM to bundle calls that can be executed simultaneously,\\nenabling external compilers to optimize these batches for\\nparallel execution. Sequential function call optimizations in-\\nclude methods like function call fusion (Singh et al., 2024a),\\ncaching (Singh et al., 2024b), compact syntax for call rep-\\nresentation (Chen et al., 2023), and partial execution of\\nfunction calls (Xu et al., 2024), which allow overlapping of\\ngeneration and execution of a single code block.\\nLimitations of a synchronous interaction.\\nCurrently,\\nLLMs perform synchronous function calls. This interleaved\\nnature of generation and execution introduces extra over-\\nheads in LLM inference, due to the stateless nature of LLMs,\\n2'),\n",
       "  Document(id='01701929-86c7-46be-bc58-269dd396f7b6', metadata={'Published': '2024-12-09', 'Title': 'Asynchronous LLM Function Calling', 'Authors': 'In Gim, Seung-seob Lee, Lin Zhong', 'Summary': \"Large language models (LLMs) use function calls to interface with external\\ntools and data source. However, the current approach to LLM function calling is\\ninherently synchronous, where each call blocks LLM inference, limiting LLM\\noperation and concurrent function execution. In this work, we propose AsyncLM,\\na system for asynchronous LLM function calling. AsyncLM improves LLM's\\noperational efficiency by enabling LLMs to generate and execute function calls\\nconcurrently. Instead of waiting for each call's completion, AsyncLM introduces\\nan interrupt mechanism to asynchronously notify the LLM in-flight when function\\ncalls return. We design an in-context protocol for function calls and\\ninterrupts, provide fine-tuning strategy to adapt LLMs to the interrupt\\nsemantics, and implement these mechanisms efficiently on LLM inference process.\\nWe demonstrate that AsyncLM can reduce end-to-end task completion latency from\\n1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks\\nin the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss\\nhow interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM\\ninteractions.\"}, page_content='speedup of up to 2.1× over synchronous parallel function\\ncalling. We also assess the impact of AsyncLM on function\\ncalling accuracy, demonstrating that AsyncLM maintains\\nthe same level of accuracy as synchronous function calling\\nin fine-tuned Llama models. Notably, GPT-4o can handle\\nasynchronous function calling without explicit fine-tuning.\\nFinally, we discuss the potential of the introduced inter-\\nrupt mechanism for use in novel AI applications involving\\nhuman-LLM or LLM-LLM interactions, such as interrupt-\\nible LLM assistants.\\n2\\nBACKGROUND AND RELATED WORK\\nThe concept of augmenting LLMs with code execution (Mi-\\nalon et al., 2023) has been extensively explored, notably\\nin contexts like retrieval-augmented generation (Khattab\\net al., 2022; Yao et al., 2023), autonomous agents (Huang\\net al., 2024; Wang et al., 2024), and neurosymbolic problem\\nsolving (Pan et al., 2023; Trinh et al., 2024).\\nLearning to generate function calls. The most common\\nmethod for LLM interaction with external systems is tool\\nor function calling (Schick et al., 2024; Shen et al., 2024),\\nwhere the model autonomously generates calls to exter-\\nnal executors (e.g., API servers, code interpreters). This\\nability is refined either through fine-tuning on diverse task\\ndatasets (Patil et al., 2024) or using in-context instruc-\\ntions (Liang et al., 2024). Function descriptions, includ-'),\n",
       "  Document(id='9ec32472-b3d7-4949-999b-f55ba2f6676a', metadata={'Published': '2024-12-09', 'Title': 'Asynchronous LLM Function Calling', 'Authors': 'In Gim, Seung-seob Lee, Lin Zhong', 'Summary': \"Large language models (LLMs) use function calls to interface with external\\ntools and data source. However, the current approach to LLM function calling is\\ninherently synchronous, where each call blocks LLM inference, limiting LLM\\noperation and concurrent function execution. In this work, we propose AsyncLM,\\na system for asynchronous LLM function calling. AsyncLM improves LLM's\\noperational efficiency by enabling LLMs to generate and execute function calls\\nconcurrently. Instead of waiting for each call's completion, AsyncLM introduces\\nan interrupt mechanism to asynchronously notify the LLM in-flight when function\\ncalls return. We design an in-context protocol for function calls and\\ninterrupts, provide fine-tuning strategy to adapt LLMs to the interrupt\\nsemantics, and implement these mechanisms efficiently on LLM inference process.\\nWe demonstrate that AsyncLM can reduce end-to-end task completion latency from\\n1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks\\nin the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss\\nhow interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM\\ninteractions.\"}, page_content='identifier, and the body of function call. The rest of this\\n3\\nflorist           search_nearby(‘flowers’) \\nput(‘msg.txt’, ‘Hi I am inquring if a ....’) \\nflorist           FastFlower, ‘313-7343’ ... \\n                    ...\\ntext(‘313-7343’, ‘msg.txt’)\\ntext(‘533-3313’, ‘msg.txt’)\\nFind local florists nearby and text \\nthem to ask about the prices for \\nMother’s Day flowers.\\nExecutor\\neos\\nhead\\nend\\ncall\\ncall\\ncall\\ncall\\nintr\\nend\\nend\\nend\\nend\\nhead\\nWeb API\\nLLM\\nYou\\nAll done!\\nAll done!\\nAssistant\\nPhone API\\nUser\\nTime\\nf1\\nf2\\nf3\\nf4\\nFigure 3. Asynchronous function execution workflow. The LLM generates function calls in CML format, which are monitored real-time\\nand sent to the code executor in the background. When a function call completes, the executor asynchronously notifies the LLM by\\ninserting an interrupt along with the execution result.\\nsection defines the semantics of CML.\\n3.1\\nInitiating Function Calls'),\n",
       "  Document(id='5e347dcb-a09c-43e8-aeb4-5d3d929a536e', metadata={'Published': '2024-12-09', 'Title': 'Asynchronous LLM Function Calling', 'Authors': 'In Gim, Seung-seob Lee, Lin Zhong', 'Summary': \"Large language models (LLMs) use function calls to interface with external\\ntools and data source. However, the current approach to LLM function calling is\\ninherently synchronous, where each call blocks LLM inference, limiting LLM\\noperation and concurrent function execution. In this work, we propose AsyncLM,\\na system for asynchronous LLM function calling. AsyncLM improves LLM's\\noperational efficiency by enabling LLMs to generate and execute function calls\\nconcurrently. Instead of waiting for each call's completion, AsyncLM introduces\\nan interrupt mechanism to asynchronously notify the LLM in-flight when function\\ncalls return. We design an in-context protocol for function calls and\\ninterrupts, provide fine-tuning strategy to adapt LLMs to the interrupt\\nsemantics, and implement these mechanisms efficiently on LLM inference process.\\nWe demonstrate that AsyncLM can reduce end-to-end task completion latency from\\n1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks\\nin the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss\\nhow interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM\\ninteractions.\"}, page_content='els (LLMs) to access external data sources and tools, such\\nas weather forecasts and calculators. Both commercial and\\nopen-source LLMs have integrated this feature (Schick et al.,\\n2024; Patil et al., 2024), unlocking new possibilities for di-\\nverse applications, from autonomous AI agents operating in\\ndynamic environments (Wang et al., 2024) to neurosymbolic\\nsystems combining symbolic reasoning with LLMs to solve\\ncomplex problems (Trinh et al., 2024).\\nLLM function calls are synchronous, with the LLM and the\\nfunction call executor taking turns generating and execut-\\ning calls. Although simple to implement, this approach is\\nneither resource-efficient nor responsive. Each function call\\nblocks LLM inference—one of the most resource-intensive\\nprocesses—until the function returns. From the executor’s\\nperspective, this limits concurrency since all function calls\\nmust finish in the order they are initiated by the LLM. These\\ninefficiencies worsen as the number of functions increases\\nwith the complexity of the task (Zaharia et al., 2024).\\nSeveral studies have tried to address these challenges, includ-\\ning using compilers to parallelize function calls (Kim et al.,\\n2023), fusing sequential calls to reduce overhead (Singh\\net al., 2024a), designing concise call syntax (Chen et al.,\\n2023), and optimizing LLM serving systems for function\\n1Department\\nof\\nComputer\\nScience,\\nYale\\nUniversity,\\nNew Haven, United States.\\nCorrespondence to:\\nIn Gim')]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"What is Function Calling in the context of AI?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7pQDUhUnIo8"
   },
   "source": [
    "## LangGraph - Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
    "\n",
    "### Why Cycles?\n",
    "\n",
    "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
    "\n",
    "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
    "\n",
    "### Why LangGraph?\n",
    "\n",
    "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRyQmEAVzua"
   },
   "source": [
    "##  Creating our Tool Belt\n",
    "\n",
    "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
    "\n",
    "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
    "\n",
    "We'll leverage:\n",
    "\n",
    "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
    "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)\n",
    "- [Wikipedia](https://python.langchain.com/docs/integrations/tools/wikipedia/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "lAxaSvlfIeOg"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "tool_belt = [\n",
    "    DuckDuckGoSearchRun(),\n",
    "    ArxivQueryRun(),\n",
    "    WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FdOjEslXdRR"
   },
   "source": [
    "### Actioning with Tools\n",
    "\n",
    "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
    "\n",
    "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "cFr1m80-JZsD"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_executor = ToolNode(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-C669ZYVI5"
   },
   "source": [
    "### Model\n",
    "\n",
    "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
    "\n",
    "- OpenAI's GPT-3.5 and GPT-4\n",
    "- Anthropic's Claude\n",
    "- Google's Gemini\n",
    "\n",
    "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "QkNS8rNZJs4z"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugkj3GzuZpQv"
   },
   "source": [
    "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "4OdMqFafZ_0V"
   },
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
    "model = model.bind_tools(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_296Ub96Z_H8"
   },
   "source": [
    "## Putting the State in Stateful\n",
    "\n",
    "Earlier we used this phrasing:\n",
    "\n",
    "`coordinated multi-actor and stateful applications`\n",
    "\n",
    "So what does that \"stateful\" mean?\n",
    "\n",
    "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
    "\n",
    "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
    "\n",
    "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
    "\n",
    "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
    "\n",
    "1. We initialize our state object:\n",
    "  - `{\"messages\" : []}`\n",
    "2. Our user submits a query to our application.\n",
    "  - New State: `HumanMessage(#1)`\n",
    "  - `{\"messages\" : [HumanMessage(#1)}`\n",
    "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
    "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
    "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
    "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "mxL9b_NZKUdL"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWsMhfO9grLu"
   },
   "source": [
    "## It's Graphing Time!\n",
    "\n",
    "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
    "\n",
    "Let's take a second to refresh ourselves about what a graph is in this context.\n",
    "\n",
    "Graphs, also called networks in some circles, are a collection of connected objects.\n",
    "\n",
    "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
    "\n",
    "Let's look at a simple graph.\n",
    "\n",
    "![image](https://i.imgur.com/2NFLnIc.png)\n",
    "\n",
    "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
    "\n",
    "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
    "\n",
    "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
    "\n",
    "Let's create some nodes and expand on our diagram.\n",
    "\n",
    "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "91flJWtZLUrl"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "# note for future: new format for kwargs: additional_kwargs={'tool_calls': [{'id': 'call_84K2HOMGkf6Cot7MF4OUeyKm', 'function': {'arguments': '{\"query\":\"Function Calling in Large Language Models\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None},\n",
    "def call_tool(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  action = ToolInvocation(\n",
    "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "      tool_input=json.loads(\n",
    "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "      )\n",
    "  )\n",
    "\n",
    "  response = tool_executor.invoke(action)\n",
    "\n",
    "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "  return {\"messages\" : [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bwR7MgWj3Wg"
   },
   "source": [
    "Now we have two total nodes. We have:\n",
    "\n",
    "- `call_model` is a node that will...well...call the model\n",
    "- `call_tool` is a node which will call a tool\n",
    "\n",
    "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "_vF4_lgtmQNo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d9e9e50>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8CjRlbVmRpW"
   },
   "source": [
    "Let's look at what we have so far:\n",
    "\n",
    "![image](https://i.imgur.com/md7inqG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaXHpPeSnOWC"
   },
   "source": [
    "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "YGCbaYqRnmiw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d9e9e50>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUsfGoSpoF9U"
   },
   "source": [
    "![image](https://i.imgur.com/wNixpJe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q_pQgHmoW0M"
   },
   "source": [
    "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
    "\n",
    "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
    "\n",
    "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
    "\n",
    "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
    "\n",
    "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "1BZgb81VQf9o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d9e9e50>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  # old: additional_kwargs={'function_call': {'arguments': '{\"query\":\"RAG in the context of Large Language Models\"}', 'name': 'duckduckgo_search'}},\n",
    "  # new: additional_kwargs={'tool_calls': [{'id': 'call_84K2HOMGkf6Cot7MF4OUeyKm', 'function': {'arguments': '{\"query\":\"Function Calling in Large Language Models\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None},\n",
    "  if \"function_call\" not in last_message.additional_kwargs:\n",
    "    return \"end\"\n",
    "\n",
    "  return \"continue\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cvhcf4jp0Ce"
   },
   "source": [
    "Let's visualize what this looks like.\n",
    "\n",
    "![image](https://i.imgur.com/8ZNwKI5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCjWJCkrJb9"
   },
   "source": [
    "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "UvcgbHf1rIXZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d9e9e50>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiWDwBQtrw7Z"
   },
   "source": [
    "Let's look at the final visualization.\n",
    "\n",
    "![image](https://i.imgur.com/NWO7usO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYqDpErlsCsu"
   },
   "source": [
    "All that's left to do now is to compile our workflow - and we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "zt9-KS8DpzNx"
   },
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYcTShCsPaa"
   },
   "source": [
    "## Using Our Graph\n",
    "\n",
    "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
    "\n",
    "Let's try out a few examples to see how it fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn4n37PQRPII",
    "outputId": "40b6f2cd-540f-42ba-a5c3-e5533d3fb8c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Function Calling in the context of Large Language Models? When did it break onto the scene?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XmjKHv3Le6MyyUm5df9QwX5S', 'function': {'arguments': '{\"query\":\"Function Calling in Large Language Models\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 231, 'total_tokens': 250, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d943b0b3-3107-4eeb-a135-0c7a0f2a29b9-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Function Calling in Large Language Models'}, 'id': 'call_XmjKHv3Le6MyyUm5df9QwX5S', 'type': 'tool_call'}], usage_metadata={'input_tokens': 231, 'output_tokens': 19, 'total_tokens': 250, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"What is Function Calling in the context of Large Language Models? When did it break onto the scene?\")]}\n",
    "\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBHnUtLSscRr"
   },
   "source": [
    "Let's look at what happened:\n",
    "\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
    "5. The agent node added a response to the state object and passed it along the conditional edge\n",
    "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXGeN-C8mbBM"
   },
   "source": [
    "## Agentic RAG with LangGraph and LCEL\n",
    "\n",
    "Let's see what our final graph will look like:\n",
    "\n",
    "![image](https://i.imgur.com/aL4Qs0E.png)\n",
    "\n",
    "Now that we have our two major components, let's create our final agent!\n",
    "\n",
    "Since we can add any LCEL `Runnable` to our graph as a node directly - we can add our RAG chain as a node with no additional steps!\n",
    "\n",
    "However, since our `Runnable` was set-up without knowledge of our state object - we need to add some pre/post processing steps to ensure it fits into the flow!\n",
    "\n",
    "> NOTE: There is only one cycle in this graph, as we cannot reach the RAG chain after the initial attempt to use it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "MlCwP-iXojru"
   },
   "outputs": [],
   "source": [
    "def convert_state_to_query(state_object):\n",
    "  return {\"question\" : state_object[\"messages\"][-1].content}\n",
    "\n",
    "def convert_response_to_state(response):\n",
    "  return {\"messages\" : [response[\"response\"]]}\n",
    "\n",
    "langgraph_node_rag_chain = convert_state_to_query | retrieval_augmented_generation_chain | convert_response_to_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTXZtCgouJ2z"
   },
   "source": [
    "Let's test our our new chain and verify it works as expected!\n",
    "\n",
    "> NOTE: We are still able to take advantage of the benefits of built in `async` provided by LCEL with this chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ku8Vk0krNU_",
    "outputId": "0d8d9464-215d-412a-8d02-25f1a7145782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='Function calling in the context of Large Language Models (LLMs) refers to how LLMs use function calls to interact with external tools and data sources. The current approach to LLM function calling is synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. The introduction of AsyncLM, proposed in a recent work, enables asynchronous LLM function calling, allowing LLMs to generate and execute function calls concurrently. AsyncLM introduces an interrupt mechanism to notify the LLM in-flight asynchronously when function calls return, improving operational efficiency and reducing end-to-end task completion latency. AsyncLM broke onto the scene in 2024, as described in the document.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 2732, 'total_tokens': 2871, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b11e55c-1a52-4efd-a7b8-3f8d4dadc325-0', usage_metadata={'input_tokens': 2732, 'output_tokens': 139, 'total_tokens': 2871, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await langgraph_node_rag_chain.ainvoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqz0HD_-uVLq"
   },
   "source": [
    "Now we'll add our nodes - notice that we're including our newly built LCEL component as a node called `first_action`.\n",
    "\n",
    "The basic idea is that we will use our private RAG set-up - and if that is deemed sufficient, we will return that response to our user; and if not we will augment our response will the other tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "RvMuxumArtT7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d76d750>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent = StateGraph(AgentState)\n",
    "\n",
    "rag_agent.add_node(\"agent\", call_model)\n",
    "rag_agent.add_node(\"action\", call_tool)\n",
    "rag_agent.add_node(\"first_action\", langgraph_node_rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaxHeBtsz-JF"
   },
   "source": [
    "Let's set our new entry point to be our RAG pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "UpHI3VhDr72t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d76d750>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent.set_entry_point(\"first_action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-GFjQen0BeH"
   },
   "source": [
    "Because we wish to have this conditional behaviour (\"is the question fully answered by the RAG pipeline?\") we'll need to add a new conditional node!\n",
    "\n",
    "We'll start by describing a process by which we can ask the question: \"Is this question fully answered by the response?\"\n",
    "\n",
    "This will let us boil down our paths to \"Yes, it is fully answered\", and \"No, it is not fully answered\".\n",
    "\n",
    "The function below will do exactly that by leaning on Pydantic and GPT-4!\n",
    "\n",
    "> NOTE: We now have an LCEL component as a node, and we have a chain *inside a function in as a node*. LangGraph is an extremely flexible framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "Z-QZVNURvDTe"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "def is_fully_answered(state):\n",
    "\n",
    "  ### Extract the question and response from our RAG pipeline\n",
    "  question = state[\"messages\"][0].content\n",
    "  answer = state[\"messages\"][-1].content\n",
    "\n",
    "  ### Create a Pydantic model to capture our LLMs response\n",
    "  class answered(BaseModel):\n",
    "    binary_score: str = Field(description=\"Fully answered: 'yes' or 'no'\")\n",
    "\n",
    "  ### A powerful reasoning model will ensure we can answer our question properly\n",
    "  model = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=0)\n",
    "\n",
    "  ### Create and bind our tool to our model\n",
    "  answered_tool = convert_to_openai_tool(answered)\n",
    "\n",
    "  model = model.bind(\n",
    "      tools=[answered_tool],\n",
    "      tool_choice={\"type\" : \"function\", \"function\" : {\"name\" : \"answered\"}}\n",
    "  )\n",
    "\n",
    "  ### We'll want to parse the output into a usable format\n",
    "  parser_tool = PydanticToolsParser(tools=[answered])\n",
    "\n",
    "  prompt = PromptTemplate(\n",
    "      template=\"\"\"You will determine if the question is fully answered by the response.\\n\n",
    "      Question:\n",
    "      {question}\n",
    "\n",
    "      Response:\n",
    "      {answer}\n",
    "\n",
    "      You will respond with either 'yes' or 'no'.\"\"\",\n",
    "      input_variables=[\"question\", \"answer\"])\n",
    "\n",
    "  ### Classic LCEL chain!\n",
    "  fully_answered_chain = prompt | model | parser_tool\n",
    "\n",
    "  response = fully_answered_chain.invoke({\"question\" : question, \"answer\" : answer})\n",
    "\n",
    "  if response[0].binary_score == \"no\":\n",
    "    return \"continue\"\n",
    "\n",
    "  return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeG_LWxn1PJJ"
   },
   "source": [
    "Let's map and add that conditional edge now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "R83jwrx8xKVf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d76d750>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent.add_conditional_edges(\n",
    "    \"first_action\",\n",
    "    is_fully_answered,\n",
    "    {\n",
    "        \"continue\" : \"agent\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFyVeahl1fdj"
   },
   "source": [
    "We'll still use our original prompt to determine if we need to use more tools or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "WD8wC2W6r_tb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d76d750>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if \"function_call\" not in last_message.additional_kwargs:\n",
    "    return \"end\"\n",
    "\n",
    "  return \"continue\"\n",
    "\n",
    "rag_agent.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1BR2VMW1k7E"
   },
   "source": [
    "Let's define the final edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "qvQzFbtJsDrE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x30d76d750>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_agent.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_8ZHvch1mqB"
   },
   "source": [
    "Time to compile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "Bgho8n0QsO_s"
   },
   "outputs": [],
   "source": [
    "rag_agent_app = rag_agent.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu069aIF1pAU"
   },
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sItf-jgY1rMU",
    "outputId": "ddac59ca-111a-42c8-d03b-e7ee033a98f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Function Calling in the Context of LLM?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Function Calling in the context of LLM refers to how large language models utilize function calls to interact with external tools and data sources. The current approach to LLM function calling is synchronous, where each call blocks LLM inference. To address this limitation, an asynchronous system called AsyncLM has been proposed to enable LLMs to generate and execute function calls concurrently. Instead of waiting for each call's completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. This improves LLM's operational efficiency and reduces end-to-end task completion latency compared to synchronous function calling.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 2704, 'total_tokens': 2826, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-db342d83-74fe-4b83-94d4-5be561e65fdb-0', usage_metadata={'input_tokens': 2704, 'output_tokens': 122, 'total_tokens': 2826, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Function Calling in the Context of LLM?\"\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
    "\n",
    "rag_agent_app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHj-i2Cc1uud"
   },
   "source": [
    "Notice how we didn't enter into our tool cycle as the query was fully answered by our baseline RAG system!\n",
    "\n",
    "Let's try another example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzQY4SSvsSE1",
    "outputId": "726c8b20-c965-447a-d99c-f3ddcd91a853"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Function Calling in the Context of LLM and when it was introduced?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Function Calling in the context of LLM refers to the capability that allows large language models to access external tools and data sources by generating and executing function calls. The traditional approach to LLM function calling is synchronous, where each call blocks LLM inference until the function returns, limiting operational efficiency and concurrent function execution. Asynchronous LLM function calling was introduced in a work called AsyncLM, which enables LLMs to generate and execute function calls concurrently and introduces an interrupt mechanism to notify the LLM asynchronously when function calls return. AsyncLM was proposed to improve LLM's operational efficiency by reducing end-to-end task completion latency compared to synchronous function calling. It was introduced in 2024.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 2709, 'total_tokens': 2848, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e8e836bc-1ea7-4752-9224-cffc4db5117f-0', usage_metadata={'input_tokens': 2709, 'output_tokens': 139, 'total_tokens': 2848, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_r6ctlc0Ubk09hQ1eItlexlL2', 'function': {'arguments': '{\"query\":\"AsyncLM\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 369, 'total_tokens': 384, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f68b49b6-714a-4cbc-a824-aac5f3a50316-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'AsyncLM'}, 'id': 'call_r6ctlc0Ubk09hQ1eItlexlL2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 369, 'output_tokens': 15, 'total_tokens': 384, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Function Calling in the Context of LLM and when it was introduced?\"\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
    "\n",
    "rag_agent_app.invoke(inputs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
