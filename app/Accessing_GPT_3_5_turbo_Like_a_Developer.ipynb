{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQt-gyAYUbm3"
   },
   "source": [
    "### Using the OpenAI Library to Programmatically Access GPT-3.5-turbo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4SKfBCSB8ds",
    "outputId": "db790e6e-5133-4565-8f53-97b0a3fcfe6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai cohere tiktoken -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PInACkIWUhOd"
   },
   "source": [
    "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecnJouXnUgKv",
    "outputId": "96d54b76-5844-465d-ae11-962d46019b86"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1pOrbwSU5H_"
   },
   "source": [
    "### Our First Prompt\n",
    "\n",
    "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) if you get stuck!\n",
    "\n",
    "Let's create a `ChatCompletion` model to kick things off!\n",
    "\n",
    "There are three \"roles\" available to use:\n",
    "\n",
    "- `system`\n",
    "- `assistant`\n",
    "- `user`\n",
    "\n",
    "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)\n",
    "\n",
    "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
    "\n",
    "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iy_LEPNEMVvC"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofMwuUQOU4sf",
    "outputId": "fc7012a7-e315-486f-b906-10c13dadcf87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AdIdwNhNDbpmWF95k3j4UKslprk9r', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two different platforms serving different purposes within the blockchain and cryptocurrency space.\\n\\nLangChain is a blockchain platform that focuses on providing language services and solutions to bridge the language gap in the global blockchain industry. It offers translation, interpretation, and language localization services for companies, projects, and individuals operating in the blockchain and cryptocurrency sector.\\n\\nLlamaIndex, on the other hand, is a cryptocurrency price index platform that provides real-time data and analysis on various digital assets. It tracks the prices, market capitalization, trading volume, and other key metrics of different cryptocurrencies to help users make informed investment decisions.\\n\\nIn summary, LangChain specializes in language services for the blockchain industry, while LlamaIndex focuses on providing cryptocurrency price data and analysis.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733929820, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=151, prompt_tokens=19, total_tokens=170, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX-7MnFhVNoT"
   },
   "source": [
    "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
    "\n",
    "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB76LJrDVgbc"
   },
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-vmtUV7WVOLW"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osXgB_5nVky_"
   },
   "source": [
    "### Testing Helper Functions\n",
    "\n",
    "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
    "\n",
    "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "4yRwAWvgWFNq",
    "outputId": "e871452b-0e60-4008-f700-7b1485a12641"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is a blockchain platform designed for language-related applications, such as translation services, language learning, and content creation. It aims to facilitate the seamless integration of language-related services through blockchain technology.\n",
       "\n",
       "On the other hand, LlamaIndex is a decentralized platform that provides users with access to a wide range of data points related to cryptocurrency markets. It offers tools for users to analyze market trends, track prices, and make informed investment decisions.\n",
       "\n",
       "In summary, LangChain is focused on language-related applications on the blockchain, while LlamaIndex is focused on providing cryptocurrency market data and analysis tools."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPs3ScS1WpoC"
   },
   "source": [
    "Let's focus on extending this a bit, and incorporate a `system` message as well!\n",
    "\n",
    "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
    "\n",
    ">REMINDER: The system message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "aSX2F3bDWYgy",
    "outputId": "05b92e0a-38f0-4ff7-ef5c-b3c2a3383980"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have any preference because I can't eat anything! I'm just a computer program here to assist you. Now, can we please move on to something that will actually fill my virtual stomach?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFs56KVaXuEY"
   },
   "source": [
    "Let's try that same prompt again, but modify only our system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "CGOlxfcFXxJ7",
    "outputId": "d4ed7b30-36f4-4472-bd6d-c251bc5293e6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm glad you're having an awesome day! As an AI, I don't have preferences, but I think both crushed ice and cubed ice have their own unique charm depending on what you're using them for. What's your preference?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkmjJd8zYQUK"
   },
   "source": [
    "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6b6z3CkYX9Y",
    "outputId": "9106bf16-b795-4dbf-feeb-890033d82ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AdInuj4pbRs7uWoHRbhCI3aMwxgJG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm glad you're having an awesome day! As an AI, I don't have preferences, but I think both crushed ice and cubed ice have their own unique charm depending on what you're using them for. What's your preference?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733930438, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=49, prompt_tokens=30, total_tokens=79, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqMRJLbOYcwq"
   },
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
    "\n",
    "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
    "\n",
    "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "iLfNEH8Fcs6c",
    "outputId": "a2ec4d1d-c830-42dc-cb71-5479193212d3"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I need to simplify this recipe because it's too stimple for my limited cooking skills, but I think adding a touch of falbean will make it more exciting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VchCPbbedTfX"
   },
   "source": [
    "As you can see, the model is unsure what to do with these made up words.\n",
    "\n",
    "Let's see if we can use the `assistant` role to show the model what these words mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "4InUN_ArZJpa",
    "outputId": "91f341ea-0dd8-44c9-9d5e-df6b10ba8322"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I need a stimple falbean to tighten this bolt properly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0zn9-X2d23Z"
   },
   "source": [
    "As you can see, leveraging the `assistant` role makes for a stimple experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWUvXSWpeCs6"
   },
   "source": [
    "### Chain of Thought Prompting\n",
    "\n",
    "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
    "\n",
    "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
    "\n",
    "Let's look at a simple reasoning based example without CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "cwW0IgbfeTwP",
    "outputId": "b4e3d706-4e08-402b-c308-446b53f980dc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, it does matter which travel option Billy selects. If he flies and then takes a bus, it will take him a total of 5 hours to get home, which means he will arrive at 6PM local time. If he takes the teleporter and then a bus, it will only take him a total of 1 hour to get home, which means he will arrive at 2PM local time. Therefore, if Billy wants to get home before 7PM EDT, he should choose the teleporter option."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFcrU-4pgRBS"
   },
   "source": [
    "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
    "\n",
    "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "HpKEt7Z5fo4s",
    "outputId": "c25a982f-afd5-4154-b2a2-388029f3e4e9"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, it does matter which travel option Billy selects.\n",
       "\n",
       "If Billy chooses to fly, it will take him 3 hours to arrive at his destination. Considering the time it takes to fly and then take a bus, he will likely not make it home before 7 PM EDT since it is currently 1 PM local time.\n",
       "\n",
       "On the other hand, if Billy takes the teleporter, he will arrive instantly at his destination without any travel time. Then, it would only take him 1 hour to take the bus from the teleporter location to his home. This option would likely get him home before 7 PM EDT.\n",
       "\n",
       "Therefore, based on the time constraints and the travel times of each option, it is more beneficial for Billy to choose the teleporter option in order to get home before 7 PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHH8zof-gkc1"
   },
   "source": [
    "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k9TKR1DhWI2"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that you're accessing `gpt-3.5-turbo` through an API, developer style, let's move on to creating a simple application powered by `gpt-3.5-turbo`!\n",
    "\n",
    "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) repository!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rGI1nJeqeO_"
   },
   "source": [
    "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
