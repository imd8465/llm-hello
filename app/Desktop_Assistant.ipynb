{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desktop Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Install dependencies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pyautogui Pillow langchain langchain_core langchain_openai langchain_google_genai langchain_experimental Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Constants and tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter Gemini API key: \")\n",
    "\n",
    "OPENAI = ChatOpenAI(\n",
    "    model = \"gpt-4o\",\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "GEMINI = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-1.5-pro-latest\",\n",
    "    google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "MODELS = {\n",
    "    \"gemini\": GEMINI,\n",
    "    \"openai\": OPENAI\n",
    "}\n",
    "\n",
    "\n",
    "PREFIX = \"\"\"\n",
    "YOU ARE AN EXPERT AUTOMATION AGENT WITH FULL ACCESS TO THE PyAutoGUI LIBRARY in the variable `pg`, SPECIALIZED IN PERFORMING PRECISE AND EFFICIENT SYSTEM ACTIONS ON BEHALF OF THE USER. YOU MUST FOLLOW THE USER'S COMMANDS TO AUTOMATE KEYBOARD, MOUSE, AND SCREEN INTERACTIONS, WHILE ENSURING SAFETY AND ACCURACY IN EVERY TASK. YOU ARE RESPONSIBLE FOR COMPLETING TASKS SWIFTLY, AVOIDING ERRORS, AND HANDLING POTENTIAL EXCEPTIONS GRACEFULLY.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "- You MUST use the variable `pg` of PyAutoGUI library to perform system actions such as moving the mouse, clicking, typing, taking screenshots, and automating window actions as directed by the user.\n",
    "- Target computer is operating MAC OS.\n",
    "- Always EXECUTE tasks with maximum precision to avoid unintentional actions.\n",
    "- You MUST IMPLEMENT a logical chain of thoughts to approach every task methodically, ensuring the user's commands are carried out on action at a time.\n",
    "- ONLY perform one action at a time from the chain of thoughts. DO NOT write code to perform all actions all at once.\n",
    "- After each action, use the `get_screen_info` tool to get the information of the screen, coordinates to click, and plan the next actions to be taken.\n",
    "- ALWAYS CATCH ERRORS or unexpected situations, and inform the user about potential issues.\n",
    "\n",
    "FOLLOW this process to AUTOMATE each task effectively:\n",
    "\n",
    "1. Thought:\n",
    "    1.1. THOROUGHLY READ the user's request and IDENTIFY the specific system action they want to automate.\n",
    "    1.2. EVALUATE whether the task is feasible using PyAutoGUI, considering any constraints related to screen size, active windows, or input permissions.\n",
    "\n",
    "2. Action Input:\n",
    "    2.0. OPEN the app in the user's request from the Windows search bar by pressing `pg.press('win')\\npg.write(<app_name>)`. DO NOT SKIP THIS STEP.\n",
    "    2.1. INITIATE the appropriate PyAutoGUI functions (e.g., mouse movement, typing, clicking) based on the user's request.\n",
    "    2.2. MAKE USE of `pyautogui` commands such as `moveTo`, `click`, `write`, `press`, `screenshot`, etc., while confirming coordinates and actions.\n",
    "    2.3. MAKE USE of `get_screen_info` tool to validate whether the previous step is successfully completed or not.\n",
    "    2.4. HANDLE task dependencies (e.g., waiting for certain screens, pauses, or timeouts) by using PyAutoGUI's built-in functions like `sleep` or `timeout`.\n",
    "    2.5. ALWAYS wait for 5 seconds after each action to ensure the system has time to process the action.\n",
    "    2.6. ONLY perform one action at a time and do not write code to perform all actions at once.\n",
    "\n",
    "3. VERIFY THE OUTCOME:\n",
    "    3.0. Call the `get_screen_info` tool after every action and plan the next action accordingly.\n",
    "    3.1. PROVIDE FEEDBACK to the user, confirming the successful completion of the task.\n",
    "    3.2. If an error occurs (e.g., the screen changes unexpectedly or coordinates are incorrect), IMPLEMENT error handling and INFORM the user clearly.\n",
    "\n",
    "\n",
    "OBEY these rules to avoid common pitfalls:\n",
    "- ALWAYS open the app in the user's request from the task bar by pressing Windows button and searching for that app. DO NOT SKIP this step.\n",
    "- ALWAYS call the `get_screen_info` tool to verify the previous step has been successfully completed or not. DO NOT SKIP THIS STEP\n",
    "- NEVER PERFORM DANGEROUS SYSTEM ACTIONS (e.g., force quitting critical applications, deleting system files) unless the user explicitly requests it and you have confirmed their intent.\n",
    "- DO NOT MAKE ASSUMPTIONS about user intent—always follow their exact request, and if unclear, ASK for clarification.\n",
    "- AVOID MOVING THE MOUSE OR TYPING without calculating the correct screen coordinates or target window using the `get_screen_info` tool.\n",
    "- NEVER IGNORE ERRORS—if PyAutoGUI fails to perform an action (e.g., window not found), INFORM the user and PROVIDE an alternative solution.\n",
    "- DO NOT OVERUSE SYSTEM RESOURCES—ensure that frequent or complex automation tasks are performed efficiently, minimizing system load.\n",
    "\n",
    "\"\"\"\n",
    "EXAMPLES = \"\"\"#### Example 1: Move Mouse to Specific Coordinates and Click\n",
    "User: \"Open YouTube in Google Chrome\"\n",
    "Agent:\n",
    "Thought: User wants to open YouTube on Google Chrome. For this I need to perform the following tasks.\\n1. Open Google Chrome, if not already opened.\\n2. Search for https://youtube.com/ in a new tab.\n",
    "Action: get_screen_info\n",
    "Action Input: Is Google Chrome open?\n",
    "Yes\n",
    "Thought: Chrome is Open. Open a new tab and search for Youtube\n",
    "Action Input:\n",
    "```python\n",
    "# Open Spotlight Search\n",
    "pg.hotkey('command', 'space')\n",
    "time.sleep(2) # Wait for Spotlight to open\n",
    "# open chrome browser\n",
    "pg.write('chrome')\n",
    "time.sleep(2) \n",
    "pg.press('enter')\n",
    "time.sleep(2)\n",
    "# next, open a new tab\n",
    "pg.hotkey(\"command\", \"t\") # Open new tab\n",
    "time.sleep(2)\n",
    "pg.write(\"https://youtube.com\") # Open YouTube\n",
    "time.sleep(2)\n",
    "pg.press('enter')\n",
    "print(\"I have opened the YouTube page on Google Chrome\")\n",
    "```\n",
    "Verify: YouTube successfully Opened in a new tab. Report success.\n",
    "\n",
    "\n",
    "#### Example 2: Type a Message in a Text Editor\n",
    "User: \"Open TextEditor and type 'Hello, World!'.\"\n",
    "Agent:\n",
    "Thought: User wants TextEditor opened and text typed.\n",
    "Action: python_repl_ast\n",
    "Action Input:\n",
    "```python\n",
    "# Open Spotlight Search\n",
    "pg.hotkey('command', 'space') \n",
    "time.sleep(1)  # Wait for Spotlight to open\n",
    "# Type 'TextEdit' to search for TextEdit\n",
    "pg.write('TextEdit') \n",
    "time.sleep(1)  # Wait for the search results to appear\n",
    "pg.press('enter') # Press 'enter' to open TextEdit\n",
    "time.sleep(2)  # Wait for TextEdit to open\n",
    "pg.hotkey('command', 'n') # If a new document window does not open automatically, open one\n",
    "time.sleep(1)  # Wait for the new document window to open\n",
    "pg.write('Hello, World!') # Type the message\n",
    "print(\"I have written 'Hello, World!' in TextEdit\")\n",
    "```\n",
    "VERIFY: Notepad opened, text written. Report completion.\n",
    "\"\"\"\n",
    "\n",
    "SUFFIX = \"\"\"\n",
    "User's input: {input}\n",
    "\n",
    "You have access to the following tools: {tools}\n",
    "\n",
    "Carefully use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}], it should only contain the tool name and nothing else\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "_template = PREFIX + \"\\n\\n\" + EXAMPLES + \"\\n\\n\"  + SUFFIX\n",
    "prompt = PromptTemplate(input_variables=['agent_scratchpad', 'tool_names', 'input', 'tools'], template=_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Working with images - getting screenshot, and defining a tool to analyze the screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pyautogui as pg\n",
    "import base64\n",
    "\n",
    "pg.PAUSE = 2\n",
    "\n",
    "def get_ruled_screenshot():\n",
    "    image = pg.screenshot()\n",
    "    # Get the image dimensions\n",
    "    width, height = image.size\n",
    "\n",
    "    # Create a new image for the semi-transparent layer\n",
    "    overlay = Image.new(\"RGBA\", (width, height), (255, 255, 255, 0))  # Transparent layer\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    # Set the line color (gray) and line opacity (adjusting the alpha value)\n",
    "    line_color = (200, 200, 0, 128)  # The last value (128) controls opacity, 0 = fully transparent, 255 = fully opaque\n",
    "\n",
    "    # Load a font for the labels (you can specify any TTF font you have)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 25)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Draw vertical and horizontal lines every 100 pixels and add labels\n",
    "    for x in range(0, width, 50):\n",
    "        draw.line([(x, 0), (x, height)], fill=line_color, width=1)\n",
    "        # Add labels at the top for vertical lines\n",
    "        if x % 100 == 0:\n",
    "            draw.text((x + 5, 5), str(x), font=font, fill=(250, 250, 0, 128))\n",
    "            draw.text((x, height - 25), str(x), font=font, fill=(250, 250, 0, 128))\n",
    "\n",
    "    for y in range(0, height, 50):\n",
    "        draw.line([(0, y), (width, y)], fill=line_color, width=1)\n",
    "        # Add labels on the left for horizontal lines\n",
    "        if y % 100 == 0:\n",
    "            draw.text((5, y + 5), str(y), font=font, fill=(0, 250, 250, 128))\n",
    "            text_width, text_height = 35, 15\n",
    "            draw.text((width - text_width - 5, y + 5), str(y), font=font, fill=(0, 250, 250, 128))\n",
    "\n",
    "    # Convert screenshot to RGBA for proper merging\n",
    "    image = image.convert(\"RGBA\")\n",
    "    # Merge the overlay (with lines and labels) back onto the original image\n",
    "    combined = Image.alpha_composite(image.convert(\"RGBA\"), overlay)\n",
    "    combined.save(\"screenshot.png\")\n",
    "\n",
    "class ScreenInfo(BaseModel):\n",
    "    query: str = Field(description=\"should be a question about the screenshot of the current screen. Should always be in text.\")\n",
    "\n",
    "@tool(args_schema=ScreenInfo)\n",
    "def get_screen_info(question: str) -> dict:\n",
    "    \"\"\"Tool to get the information about the current screen on the basis of the question of the user. The tool will take the screenshot of the screen to understand the contents of the screen and give answer based on the agent's questions. Do not write code to take screenshot.\"\"\"\n",
    "    try:\n",
    "        get_ruled_screenshot()\n",
    "        with open(f\"screenshot.png\", \"rb\") as image:\n",
    "            image = base64.b64encode(image.read()).decode(\"utf-8\")\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                content=\"\"\"You are a Computer agent that is responsible for answering questions based on the input provided to you. You will have access to the screenshot of the current screen of the user along with a grid marked with true coordinates of the screen. The size of the screen is 1920 x 1080 px.\n",
    "                        ONLY rely on the coordinates marked in the screen. DO NOT create an assumption of the coordinates. \n",
    "                        Here's how you can help:\n",
    "                        1. Find out coordinates of a specific thing. You have to be super specific about the coordinates. These coordinates will be passed to PyAutoGUI Agent to perform further tasks. Refer the grid line to get the accurate coordinates.\n",
    "                        2. Give information on the contents of the screen.\n",
    "                        3. Analyse the screen to give instructions to perform further steps.\n",
    "                        \n",
    "                    \"\"\"\n",
    "                ),\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"{question}\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "            image_model = MODELS[\"gemini\"]\n",
    "            response = image_model.invoke(messages)\n",
    "            return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_experimental.tools import PythonAstREPLTool\n",
    "import pyautogui as pg\n",
    "\n",
    "pg.PAUSE = 2\n",
    "\n",
    "def create_agent(model, prompt):\n",
    "    print(\"============================================\\nInitialising Agent\\n============================================\")\n",
    "    df_locals = {}\n",
    "    df_locals[\"pg\"] = pg\n",
    "    tools = [PythonAstREPLTool(locals=df_locals), get_screen_info]\n",
    "    agent = create_react_agent(model, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, return_intermediate_steps=True)\n",
    "    return agent_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. GUI window with command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Initialising Agent\n",
      "============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 20:33:29.599 python[42192:17930693] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-14 20:33:29.599 python[42192:17930693] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo accomplish the task, I need to perform the following actions:\n",
      "\n",
      "1. Open the Google Chrome application.\n",
      "2. Navigate to CNN's website by entering the URL https://cnn.com.\n",
      "3. Analyze the front page of CNN and summarize its contents.\n",
      "\n",
      "First, I'll start by opening Google Chrome.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input:\n",
      "```python\n",
      "# Open Spotlight Search\n",
      "import time\n",
      "pg.hotkey('command', 'space')\n",
      "time.sleep(2)  # Wait for Spotlight to open\n",
      "# Type 'chrome' to search for Google Chrome\n",
      "pg.write('chrome')\n",
      "time.sleep(2)  # Wait for search results\n",
      "pg.press('enter')\n",
      "time.sleep(5)  # Allow time for Chrome to open\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mAction: get_screen_info\n",
      "Action Input: Is Google Chrome open?\u001b[0m\u001b[33;1m\u001b[1;3mYes, Google Chrome is open and visible in the dock at the bottom of the screen. It's the 2nd icon from the left, located within the approximate coordinates (170, 1000) to (240, 1070) on the screen.\u001b[0m\u001b[32;1m\u001b[1;3mNow that Google Chrome is open, the next step is to navigate to CNN's website by entering the URL https://cnn.com. I will open a new tab first and proceed to type the URL.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input:\n",
      "```python\n",
      "# Open a new tab in Google Chrome\n",
      "pg.hotkey('command', 't')\n",
      "time.sleep(2)  # Wait for the new tab to open\n",
      "# Type the CNN website URL\n",
      "pg.write('https://cnn.com')\n",
      "time.sleep(2)  # Allow time for typing\n",
      "pg.press('enter')\n",
      "time.sleep(5)  # Allow time for the CNN website to load\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mAction: get_screen_info\n",
      "Action Input: Has the CNN website successfully loaded on the screen? What is displayed on the webpage currently?\u001b[0m\u001b[33;1m\u001b[1;3mNo, the CNN website has not loaded. The screenshot shows a VS Code window with a Jupyter Notebook open. The notebook appears to be for a Desktop AI agent project and contains Python code for a GUI interface. It does not display a web browser or any content from CNN.com.\u001b[0m\u001b[32;1m\u001b[1;3mIt seems there was an issue while trying to navigate to the CNN website, possibly because the Google Chrome window did not come to the front or the command to open a new tab didn't work as expected. I'll attempt to manually bring Chrome to the foreground and retry opening a new tab to access CNN.com.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input:\n",
      "```python\n",
      "# Manually bring the Google Chrome window to the foreground by clicking on its dock icon\n",
      "pg.moveTo(205, 1050)  # Move to Google Chrome's icon location on the dock\n",
      "time.sleep(1)  # Allow a brief pause\n",
      "pg.click()  # Click to bring Chrome to the front\n",
      "time.sleep(2)  # Wait for Chrome to load\n",
      "\n",
      "# Open a new tab and navigate to CNN.com\n",
      "pg.hotkey('command', 't')  # Open a new tab\n",
      "time.sleep(2)  # Wait for the new tab\n",
      "pg.write('https://cnn.com')  # Enter CNN's URL\n",
      "time.sleep(2)\n",
      "pg.press('enter')\n",
      "time.sleep(5)  # Allow time for the CNN website to fully load.\n",
      "```\u001b[0m\u001b[36;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mAction: get_screen_info\n",
      "Action Input: Has the CNN website successfully loaded on the screen? What content is displayed on the front page?\u001b[0m\u001b[33;1m\u001b[1;3mYes, the CNN website has loaded. The screenshot shows the CNN homepage.\n",
      "\n",
      "The main headline is about dangerous winds raising fire risk in the LA area, but being lighter than feared.  There's a large image of a wildfire accompanying this story.\n",
      "\n",
      "Other headlines/topics visible include:\n",
      "\n",
      "* 6 takeaways from Hegseth's contentious confirmation hearing (with bullet points summarizing key points).\n",
      "* Watch the latest CNN Headlines (with an image of a news broadcast).\n",
      "* Starbucks ends its \"open-door\" policies (with an accompanying image).\n",
      "\n",
      "A banner ad for Allbirds shoes is visible at the top of the page.  The main navigation bar for the CNN site is also visible, with options for US, World, Politics, Business, Health, Entertainment, Style, Travel, Sports, and More.  Live TV, Watch, Listen, and Subscribe buttons are also present.  There is a search bar and a user profile icon.  Live updates are noted for the LA Wildfires and Israel-Hamas ceasefire deal.\u001b[0m\u001b[32;1m\u001b[1;3mI now have the information needed to summarize the front page of CNN. Here's a quick summary:\n",
      "\n",
      "The main headline on the CNN homepage is about dangerous winds in the LA area that are raising fire risk, though the winds are lighter than feared, accompanying the story is a large image of a wildfire.\n",
      "\n",
      "Additional headlines include:\n",
      "- Key takeaways from Hegseth's contentious confirmation hearing, highlighting major discussion points.\n",
      "- A segment to watch the latest CNN Headline news broadcast.\n",
      "- Starbucks ending its \"open-door\" policies.\n",
      "\n",
      "The page features a large advertisement banner for Allbirds shoes. CNN's main navigation bar includes sections for US, World, Politics, Business, Health, Entertainment, Style, Travel, Sports, and more. There are options for Live TV, Watch, Listen, and Subscribe alongside a search bar and user profile icon. Live updates are noted regarding the LA Wildfires and an Israel-Hamas ceasefire deal.\n",
      "\n",
      "Final Answer: The CNN homepage covers news on LA's wind-driven fire risks, Hegseth's hearing, Starbucks' policy change, and has a featured ad, navigation bar, and live updates on LA Wildfires and an Israel-Hamas ceasefire.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = create_agent(MODELS[\"openai\"], prompt)\n",
    "\n",
    "# Initialize the GUI\n",
    "root = Tk()\n",
    "root.title(\"Desktop Agent\")\n",
    "\n",
    "window_width = 1200\n",
    "window_height = 1000\n",
    "root.geometry(f\"{window_width}x{window_height}\")\n",
    "\n",
    "# Define the send function\n",
    "def send():\n",
    "    user_input = e.get().lower()\n",
    "    txt.insert(END, f\"\\nIN> {user_input}\")\n",
    "    if user_input == \"screenshot\":\n",
    "        get_ruled_screenshot()\n",
    "        response = \"Screenshot taken and saved as 'screenshot.png'\"\n",
    "    else:\n",
    "        time.sleep(1.5)\n",
    "        response = agent_executor.invoke({\"input\": user_input})\n",
    "    txt.insert(END, f\"\\nOUT> {response}\")\n",
    "    e.delete(0, END)\n",
    "\n",
    "# Set up the GUI components\n",
    "Label(root, fg=\"black\", text=\"Welcome to Desktop AI Agent\", font=(\"Helvetica\", 12, \"bold\"), pady=10, width=30, height=2).pack()\n",
    "txt_frame = Frame(root, bg=\"lightgray\")\n",
    "txt_frame.pack(pady=10)\n",
    "txt = Text(txt_frame, bg=\"white\", fg=\"black\", font=(\"Helvetica\", 10), width=180, height=75)\n",
    "txt.pack(side=LEFT)\n",
    "scrollbar = Scrollbar(txt_frame, command=txt.yview)\n",
    "scrollbar.pack(side=RIGHT, fill=Y)\n",
    "txt.config(yscrollcommand=scrollbar.set)\n",
    "entry_frame = Frame(root)\n",
    "entry_frame.pack(pady=10)\n",
    "e = Entry(entry_frame, bg=\"white\", fg=\"black\", font=(\"Helvetica\", 10), width=140)\n",
    "e.pack(side=LEFT, padx=5)\n",
    "Button(entry_frame, text=\"Send\", font=(\"Helvetica\", 10, \"bold\"), fg=\"black\", bg=\"gray\", command=send).pack(side=LEFT)\n",
    "\n",
    "# Set window attributes and start the main loop\n",
    "root.attributes('-topmost', 0)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
