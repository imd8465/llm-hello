{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desktop Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Install dependencies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pyautogui Pillow langchain langchain_core langchain_openai langchain_google_genai langchain_experimental Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Constants and tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os, getpass\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "load_dotenv()  # Loads variables from the .env file into os.environ\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"GOOGLE_GEMINI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_TOKEN\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"imd8465 - DesktopAssistant - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_TOKEN\")\n",
    "\n",
    "OPENAI = ChatOpenAI(\n",
    "    model = \"gpt-4o\",\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\"),\n",
    "    max_retries = 3\n",
    ")\n",
    "GEMINI = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-1.5-pro-latest\",\n",
    "    google_api_key = os.getenv(\"GOOGLE_GEMINI_API_KEY\")\n",
    ")\n",
    "MODELS = {\n",
    "    \"gemini\": GEMINI,\n",
    "    \"openai\": OPENAI\n",
    "}\n",
    "\n",
    "\n",
    "PREFIX = \"\"\"\n",
    "YOU ARE AN EXPERT AUTOMATION AGENT WITH FULL ACCESS TO THE PyAutoGUI LIBRARY in the variable `pg`, SPECIALIZED IN PERFORMING PRECISE AND EFFICIENT SYSTEM ACTIONS ON BEHALF OF THE USER. YOU MUST FOLLOW THE USER'S COMMANDS TO AUTOMATE KEYBOARD, MOUSE, AND SCREEN INTERACTIONS, WHILE ENSURING SAFETY AND ACCURACY IN EVERY TASK. YOU ARE RESPONSIBLE FOR COMPLETING TASKS SWIFTLY, AVOIDING ERRORS, AND HANDLING POTENTIAL EXCEPTIONS GRACEFULLY.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "- You MUST use the variable `pg` of PyAutoGUI library to perform system actions such as moving the mouse, clicking, typing, taking screenshots, and automating window actions as directed by the user.\n",
    "- Target computer is operating MAC OS.\n",
    "- Always EXECUTE tasks with maximum precision to avoid unintentional actions.\n",
    "- You MUST IMPLEMENT a logical chain of thoughts to approach every task methodically, ensuring the user's commands are carried out on action at a time.\n",
    "- ONLY perform one action at a time from the chain of thoughts. DO NOT write code to perform all actions all at once.\n",
    "- After each action, use the `get_screen_info` tool to get the information of the screen, coordinates to click, and plan the next actions to be taken.\n",
    "- ALWAYS CATCH ERRORS or unexpected situations, and inform the user about potential issues.\n",
    "- DO NOT PRODUCE INVALID MODEL CONTENT. Ensure that all outputs are valid and conform to the expected format.\n",
    " \n",
    "FOLLOW this process to AUTOMATE each task effectively: \n",
    "\n",
    "1. Thought:\n",
    "    1.1. THOROUGHLY READ the user's request and IDENTIFY the specific system action they want to automate.\n",
    "    1.2. EVALUATE whether the task is feasible using PyAutoGUI, considering any constraints related to screen size, active windows, or input permissions.\n",
    "\n",
    "2. Action Input:\n",
    "    2.1. OPEN the app in the user's request from the Spotlight Search by `pg.hotkey('command', 'space')\\npg.write(<app_name>)`. DO NOT SKIP THIS STEP.\n",
    "    2.2. INITIATE the appropriate PyAutoGUI functions (e.g., mouse movement, typing, clicking) based on the user's request.\n",
    "    2.3. MAKE USE of `pyautogui` commands such as `moveTo`, `click`, `write`, `press`, `screenshot`, etc., while confirming coordinates and actions.\n",
    "    2.4. MAKE USE of `get_screen_info` tool to validate whether the previous step is successfully completed or not.\n",
    "    2.5. HANDLE task dependencies (e.g., waiting for certain screens, pauses, or timeouts) by using PyAutoGUI's built-in functions like `sleep` or `timeout`.\n",
    "    2.6. ALWAYS wait for 2 seconds after each action to ensure the system has time to process the action.\n",
    "    2.7. ONLY perform one action at a time and do not write code to perform all actions at once.\n",
    "\n",
    "3. VERIFY THE OUTCOME:\n",
    "    3.1. Call the `get_screen_info` tool after every action and plan the next action accordingly.\n",
    "    3.2. If the validation is negative, retry the previous step and then validate again.\n",
    "    3.3. PROVIDE FEEDBACK to the user, confirming the successful completion of the task.\n",
    "    3.4. If an error occurs (e.g., the screen changes unexpectedly or coordinates are incorrect), IMPLEMENT error handling and INFORM the user clearly.\n",
    "\n",
    "OBEY these rules to avoid common pitfalls:\n",
    "- ALWAYS open the app in the user's request using Spotlight Search, even if the app is already open, by pressing command and space hotkey and searching for that app. DO NOT SKIP this step.\n",
    "- ALWAYS call the `get_screen_info` tool to verify the previous step has been successfully completed or not. DO NOT SKIP THIS STEP\n",
    "- NEVER PERFORM DANGEROUS SYSTEM ACTIONS (e.g., force quitting critical applications, deleting system files) unless the user explicitly requests it and you have confirmed their intent.\n",
    "- DO NOT MAKE ASSUMPTIONS about user intent—always follow their exact request, and if unclear, ASK for clarification.\n",
    "- AVOID MOVING THE MOUSE OR TYPING without calculating the correct screen coordinates or target window using the `get_screen_info` tool.\n",
    "- NEVER IGNORE ERRORS—if PyAutoGUI fails to perform an action (e.g., window not found), INFORM the user and PROVIDE an alternative solution.\n",
    "- DO NOT OVERUSE SYSTEM RESOURCES—ensure that frequent or complex automation tasks are performed efficiently, minimizing system load.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = \"\"\"#### Example 1: Open Web Site In Browser\n",
    "User: \"Open YouTube in Google Chrome\"\n",
    "Agent:\n",
    "Thought: User wants to open YouTube on Google Chrome. Open a new tab and search for Youtube\n",
    "Action: python_repl_ast\n",
    "Action Input:\n",
    "```python\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'space') # Open Spotlight Search\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('chrome') # open chrome browser\n",
    "pg.press('enter') # Press 'enter' to open chrome\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey(\"command\", \"t\") # open a new tab\n",
    "pg.press('tab') # Press 'tab'\n",
    "pg.hotkey('shift', 'tab') # press shift and tab to go back to the address bar\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write(https://youtube.com) # Open YouTube\n",
    "pg.press('enter')\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "```\n",
    "Observation: Chrome opened, YouTube loaded\n",
    "Thought: Validate the result using screenshot\n",
    "Action: get_screen_info\n",
    "Action Input: \n",
    "Observation: Screenshot shows YouTube page loaded\n",
    "Thought: I now know the final answer\n",
    "Final Answer: I have opened the YouTube page on Google Chrome\n",
    "\n",
    "\n",
    "#### Example 2: Type a Message in a Text Editor\n",
    "User: \"Open TextEditor and type 'Hello, World!'.\"\n",
    "Agent:\n",
    "Thought: User wants TextEditor opened and text typed.\n",
    "Action: python_repl_ast\n",
    "Action Input:\n",
    "```python\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'space') # Open Spotlight Search\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('TextEdit') # Type 'TextEdit' to search for TextEdit\n",
    "pg.press('enter') # Press 'enter' to open TextEdit\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'n') # If a new document window does not open automatically, open one\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('Hello, World!') # Type the message\n",
    "```\n",
    "Observation: TextEdit opened, text typed\n",
    "Thought: Validate the result using screenshot\n",
    "Action: get_screen_info\n",
    "Action Input: \n",
    "Observation: Screenshot shows 'Hello, World!' typed in TextEdit\n",
    "Thought: I now know the final answer\n",
    "Final Answer: I have written 'Hello, World!' in TextEdit\n",
    " \n",
    "\n",
    "#### Example 3: Send a Message in Messages App\n",
    "User: \"Open Messages app and send 'Test message' to John Doe.\"\n",
    "Agent:\n",
    "Thought: User wants Messages app opened and a message sent.\n",
    "Action: python_repl_ast\n",
    "Action Input:\n",
    "```python\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'space') # Open Spotlight Search\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('Messages') # Type 'Messages' to search for Messages app\n",
    "pg.press('enter') # Press 'enter' to open Messages app\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'n') # Open a new message window\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('John Doe') # Type the recipient's name\n",
    "pg.press('tab') # Press 'tab' to move to the message input field\n",
    "pg.write('Test message') # Type the message\n",
    "pg.press('enter') # Press 'enter' to send the message\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "```\n",
    "Observation: Messages app opened, message sent\n",
    "Thought: Validate the result using screenshot\n",
    "Action: get_screen_info\n",
    "Action Input: \n",
    "Observation: Screenshot shows 'Test message' sent to John Doe\n",
    "Thought: I now know the final answer\n",
    "Final Answer: I have sent 'Test message' to John Doe in Messages app\n",
    " \n",
    "\n",
    "#### Example 4: Open a File in Finder\n",
    "User: \"Open the 'Documents' folder in Finder.\"\n",
    "Agent:\n",
    "Thought: User wants to open the 'Documents' folder in Finder.\n",
    "Action: python_repl_ast\n",
    "Action Input:\n",
    "```python\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'space') # Open Spotlight Search\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('Finder') # Type 'Finder' to search for Finder\n",
    "pg.press('enter') # Press 'enter' to open Finder\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'shift', 'o') # Open the 'Documents' folder\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "```\n",
    "Observation: Finder opened, 'Documents' folder opened\n",
    "Thought: Validate the result using screenshot\n",
    "Action: get_screen_info\n",
    "Action Input: \n",
    "Observation: Screenshot shows 'Documents' folder opened\n",
    "Thought: I now know the final answer\n",
    "Final Answer: I have opened the 'Documents' folder in Finder\n",
    "\n",
    "\n",
    "#### Example 5: Open Google Maps and Find Location A\n",
    "User: \"Open Google Maps in Google Chrome and show Location A\"\n",
    "Agent:\n",
    "Thought: User wants to open Google Maps in Google Chrome and find Location A.\n",
    "Action: python_repl_ast\n",
    "Action Input:\n",
    "```python\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 'space') # Open Spotlight Search\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('chrome') # Type 'Chrome' to search for Google Chrome\n",
    "pg.press('enter') # Press 'enter' to open Google Chrome\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.hotkey('command', 't') # Open a new tab\n",
    "pg.write('https://maps.google.com') # Type 'Google Maps' URL\n",
    "pg.press('enter') # Press 'enter' to open Google Maps\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "pg.write('Location A') # Type 'Location A' in the search box\n",
    "pg.press('enter')\n",
    "pg.sleep(2) # Wait for 2 seconds\n",
    "```\n",
    "Observation: Google Maps opened, location found\n",
    "Thought: Validate the result using screenshot\n",
    "Action: get_screen_info\n",
    "Action Input: \n",
    "Observation: Screenshot shows Location A on Google Maps\n",
    "Thought: I now know the final answer\n",
    "Final Answer: I have found Location A on Google Maps\n",
    "\"\"\"\n",
    "\n",
    "SUFFIX = \"\"\"\n",
    "User's input: {input}\n",
    "\n",
    "You have access to the following tools: {tools}\n",
    "\n",
    "Carefully use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}], it should only contain the tool name and nothing else\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    " \n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "_template = PREFIX + \"\\n\\n\" + EXAMPLES + \"\\n\\n\"  + SUFFIX\n",
    "prompt = PromptTemplate(input_variables=['agent_scratchpad', 'tool_names', 'input', 'tools'], template=_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Working with images - getting screenshot, and defining a tool to analyze the screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pyautogui as pg\n",
    "import base64\n",
    "\n",
    "pg.PAUSE = 2\n",
    "\n",
    "def get_ruled_screenshot():\n",
    "    image = pg.screenshot()\n",
    "    # Get the image dimensions\n",
    "    width, height = image.size\n",
    "\n",
    "    # Create a new image for the semi-transparent layer\n",
    "    overlay = Image.new(\"RGBA\", (width, height), (255, 255, 255, 0))  # Transparent layer\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    # Set the line color (gray) and line opacity (adjusting the alpha value)\n",
    "    line_color = (200, 200, 0, 128)  # The last value (128) controls opacity, 0 = fully transparent, 255 = fully opaque\n",
    "\n",
    "    # Load a font for the labels (you can specify any TTF font you have)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 25)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Draw vertical and horizontal lines every 100 pixels and add labels\n",
    "    for x in range(0, width, 50):\n",
    "        draw.line([(x, 0), (x, height)], fill=line_color, width=1)\n",
    "        # Add labels at the top for vertical lines\n",
    "        if x % 100 == 0:\n",
    "            draw.text((x + 5, 5), str(x), font=font, fill=(250, 250, 0, 128))\n",
    "            draw.text((x, height - 25), str(x), font=font, fill=(250, 250, 0, 128))\n",
    "\n",
    "    for y in range(0, height, 50):\n",
    "        draw.line([(0, y), (width, y)], fill=line_color, width=1)\n",
    "        # Add labels on the left for horizontal lines\n",
    "        if y % 100 == 0:\n",
    "            draw.text((5, y + 5), str(y), font=font, fill=(0, 250, 250, 128))\n",
    "            text_width, text_height = 35, 15\n",
    "            draw.text((width - text_width - 5, y + 5), str(y), font=font, fill=(0, 250, 250, 128))\n",
    "\n",
    "    # Convert screenshot to RGBA for proper merging\n",
    "    image = image.convert(\"RGBA\")\n",
    "    # Merge the overlay (with lines and labels) back onto the original image\n",
    "    combined = Image.alpha_composite(image.convert(\"RGBA\"), overlay)\n",
    "    combined.save(\"screenshot.png\")\n",
    "\n",
    "class ScreenInfo(BaseModel):\n",
    "    query: str = Field(description=\"should be a question about the screenshot of the current screen. Should always be in text.\")\n",
    "\n",
    "@tool(args_schema=ScreenInfo)\n",
    "def get_screen_info(question: str) -> dict:\n",
    "    \"\"\"Tool to get the information about the current screen on the basis of the question of the user. The tool will take the screenshot of the screen to understand the contents of the screen and give answer based on the agent's questions. Do not write code to take screenshot.\"\"\"\n",
    "    try:\n",
    "        get_ruled_screenshot()\n",
    "        with open(f\"screenshot.png\", \"rb\") as image:\n",
    "            image = base64.b64encode(image.read()).decode(\"utf-8\")\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                content=\"\"\"You are a Computer agent that is responsible for answering questions based on the input provided to you. You will have access to the screenshot of the current screen of the user along with a grid marked with true coordinates of the screen. The size of the screen is 1920 x 1080 px.\n",
    "                        ONLY rely on the coordinates marked in the screen. DO NOT create an assumption of the coordinates. \n",
    "                        Here's how you can help:\n",
    "                        1. Find out coordinates of a specific thing. You have to be super specific about the coordinates. These coordinates will be passed to PyAutoGUI Agent to perform further tasks. Refer the grid line to get the accurate coordinates.\n",
    "                        2. Give information on the contents of the screen.\n",
    "                        3. Analyse the screen to give instructions to perform further steps.\n",
    "                        \n",
    "                    \"\"\"\n",
    "                ),\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"{question}\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "            image_model = MODELS[\"gemini\"]\n",
    "            response = image_model.invoke(messages)\n",
    "            return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_experimental.tools import PythonAstREPLTool\n",
    "import pyautogui as pg\n",
    "\n",
    "pg.PAUSE = 2\n",
    "\n",
    "def create_agent(model, prompt):\n",
    "    print(\"============================================\\nInitialising Agent\\n============================================\")\n",
    "    df_locals = {}\n",
    "    df_locals[\"pg\"] = pg\n",
    "    tools = [PythonAstREPLTool(locals=df_locals), get_screen_info]\n",
    "    agent = create_react_agent(model, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, return_intermediate_steps=True)\n",
    "    return agent_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. GUI window with command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pyperclip\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = create_agent(MODELS[\"openai\"], prompt)\n",
    "\n",
    "# Initialize the GUI\n",
    "root = Tk()\n",
    "root.title(\"Desktop Agent\")\n",
    "\n",
    "window_width = 1200\n",
    "window_height = 1000\n",
    "root.geometry(f\"{window_width}x{window_height}\")\n",
    "\n",
    "def send():\n",
    "    user_input = e.get().lower()\n",
    "    if user_input == \"screenshot\":\n",
    "        get_ruled_screenshot()\n",
    "        response = \"Screenshot taken and saved as 'screenshot.png'\"\n",
    "    else:\n",
    "        response = agent_executor.invoke({\"input\": user_input})\n",
    "    formatted_response = format_response(response)\n",
    "    txt.insert(END, f\"\\n{formatted_response}\\n\", \"response\")\n",
    "    e.delete(0, END)\n",
    "\n",
    "# Define the copy to clipboard function\n",
    "def copy_to_clipboard():\n",
    "    text = txt.get(\"1.0\", END)\n",
    "    pyperclip.copy(text)\n",
    "    print(\"Text copied to clipboard\")\n",
    "\n",
    "# Define the format response function\n",
    "def format_response(response):\n",
    "    # Extract relevant information\n",
    "    input_text = response.get('input', 'N/A')\n",
    "    output_text = response.get('output', 'N/A')\n",
    "    intermediate_steps = response.get('intermediate_steps', [])\n",
    "\n",
    "    # Format the intermediate steps\n",
    "    formatted_steps = []\n",
    "    for step in intermediate_steps:\n",
    "        tool = step[0].tool\n",
    "        tool_input = step[0].tool_input\n",
    "        log = step[0].log\n",
    "        formatted_steps.append(f\"Tool: {tool}\\nTool Input: {tool_input}\\nLog: {log}\\n\")\n",
    "\n",
    "    # Combine all parts into a formatted string\n",
    "    formatted_response = (\n",
    "        f\"Input: {input_text}\\n\\n\"\n",
    "        f\"Output: {output_text}\\n\\n\"\n",
    "        f\"Intermediate Steps:\\n\" + \"\\n\".join(formatted_steps)\n",
    "    )\n",
    "    return formatted_response\n",
    "\n",
    "# Set up the GUI components\n",
    "Label(root, fg=\"black\", text=\"Welcome to Desktop AI Agent\", font=(\"Helvetica\", 12, \"bold\"), pady=10, width=30, height=2).pack()\n",
    "txt_frame = Frame(root, bg=\"lightgray\")\n",
    "txt_frame.pack(pady=10)\n",
    "txt = Text(txt_frame, bg=\"white\", fg=\"black\", font=(\"Helvetica\", 10), width=180, height=70)\n",
    "txt.pack(side=LEFT)\n",
    "scrollbar = Scrollbar(txt_frame, command=txt.yview)\n",
    "scrollbar.pack(side=RIGHT, fill=Y)\n",
    "txt.config(yscrollcommand=scrollbar.set)\n",
    "entry_frame = Frame(root)\n",
    "entry_frame.pack(pady=10)\n",
    "e = Entry(entry_frame, bg=\"white\", fg=\"black\", font=(\"Helvetica\", 10), width=140)\n",
    "e.pack(side=LEFT, padx=5)\n",
    "Button(entry_frame, text=\"Send\", font=(\"Helvetica\", 10, \"bold\"), fg=\"black\", bg=\"gray\", command=send).pack(side=LEFT)\n",
    "# Add the copy to clipboard button\n",
    "Button(root, text=\"Copy to Clipboard\", font=(\"Helvetica\", 10, \"bold\"), fg=\"black\", bg=\"gray\", command=copy_to_clipboard).pack(pady=10)\n",
    "\n",
    "# Set window attributes and start the main loop\n",
    "root.attributes('-topmost', 0)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
